{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gbF71GHptuwV"
      },
      "source": [
        "# Consumer Complaints Challenge\n",
        "\n",
        "Refer to [Consumer Complaints](https://github.com/InsightDataScience/consumer_complaints) challenge from [InsightDataScience](https://github.com/InsightDataScience/) for more details. \n",
        "\n",
        "The goal of this notebook is to solve the challenge using Apache Spark. \n",
        "\n",
        "The most important sections are **Input dataset** and **Expected output**, which are quoted below:\n",
        "\n",
        "## Input dataset\n",
        "\n",
        "Below are the contents of an example `complaints.csv` file: \n",
        "```\n",
        "Date received,Product,Sub-product,Issue,Sub-issue,Consumer complaint narrative,Company public response,Company,State,ZIP code,Tags,Consumer consent provided?,Submitted via,Date sent to company,Company response to consumer,Timely response?,Consumer disputed?,Complaint ID\n",
        "2019-09-24,Debt collection,I do not know,Attempts to collect debt not owed,Debt is not yours,\"transworld systems inc. is trying to collect a debt that is not mine, not owed and is inaccurate.\",,TRANSWORLD SYSTEMS INC,FL,335XX,,Consent provided,Web,2019-09-24,Closed with explanation,Yes,N/A,3384392\n",
        "```\n",
        "Each line of the input file, except for the first-line header, represents one complaint. Consult the [Consumer Finance Protection Bureau's technical documentation](https://cfpb.github.io/api/ccdb/fields.html) for a description of each field.  \n",
        "\n",
        "* Notice that complaints were not listed in chronological order\n",
        "* In 2019, there was a complaint against `TRANSWORLD SYSTEMS INC` for `Debt collection` \n",
        "* Also in 2019, `Experian Information Solutions Inc.` received one complaint for `Credit reporting, credit repair services, or other personal consumer reports` while `TRANSUNION INTERMEDIATE HOLDINGS, INC.` received two\n",
        "* In 2020, `Experian Information Solutions Inc.` received a complaint for `Credit reporting, credit repair services, or other personal consumer reports`\n",
        "\n",
        "In summary that means \n",
        "* In 2019, there was one complaint for `Debt collection`, and 100% of it went to one company \n",
        "* Also in 2019, three complaints against two companies were received for `Credit reporting, credit repair services, or other personal consumer reports` and 2/3rd of them (or 67% if we rounded the percentage to the nearest whole number) were against one company (TRANSUNION INTERMEDIATE HOLDINGS, INC.)\n",
        "* In 2020, only one complaint was received for `Credit reporting, credit repair services, or other personal consumer reports`, and so the highest percentage received by one company would be 100%\n",
        "\n",
        "For this challenge, we want for each product and year that complaints were received, the total number of complaints, number of companies receiving a complaint and the highest percentage of complaints directed at a single company.\n",
        "\n",
        "For the purposes of this challenge, all names, including company and product, should be treated as case insensitive. For example, \"Acme\", \"ACME\", and \"acme\" would represent the same company.\n",
        "\n",
        "## Expected output\n",
        "\n",
        "Each line in the output file should list the following fields in the following order:\n",
        "* product (name should be written in all lowercase)\n",
        "* year\n",
        "* total number of complaints received for that product and year\n",
        "* total number of companies receiving at least one complaint for that product and year\n",
        "* highest percentage (rounded to the nearest whole number) of total complaints filed against one company for that product and year. Use standard rounding conventions (i.e., Any percentage between 0.5% and 1%, inclusive, should round to 1% and anything less than 0.5% should round to 0%)\n",
        "\n",
        "The lines in the output file should be sorted by product (alphabetically) and year (ascending)\n",
        "\n",
        "Given the above `complaints.csv` input file, we'd expect an output file, `report.csv`, in the following format\n",
        "```\n",
        "\"credit reporting, credit repair services, or other personal consumer reports\",2019,3,2,67\n",
        "\"credit reporting, credit repair services, or other personal consumer reports\",2020,1,1,100\n",
        "debt collection,2019,1,1,100\n",
        "```\n",
        "Notice that because `debt collection` was only listed for 2019 and not 2020, the output file only has a single entry for debt collection. Also, notice that when a product has a comma (`,`) in the name, the name should be enclosed by double quotation marks (`\"`). Finally, notice that percentages are listed as numbers and do not have `%` in them.\n",
        "\n",
        "# Objectives\n",
        "\n",
        "1. In Task 1, we work on a solution with PySpark on Google Colab using a sample of the data. The data is available on Google Drive and is to be downloaded by the `gdown` command in Task 1.\n",
        "\n",
        "2. In Task 2, we create a standalone Python script that work on the full dataset using GCP DataProc. The full dataset is downloaded from [here](https://www.consumerfinance.gov/data-research/consumer-complaints/#download-the-data). The data is available on the class bucket as: `gs://bdma/data/complaints.csv`\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VBWF5LNefozN"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjzjcPWYnHLr",
        "outputId": "207332aa-b62d-4f0f-9670-30844979784e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%shell\n",
        "gdown --quiet 1-IeoZDwT5wQzBUpsaS5B6vTaP-2ZBkam\n",
        "pip --quiet install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "XHKq10WXnZl7",
        "outputId": "d64fe13f-97c8-43b9-d6ac-57e9b81811db"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b7bcdb29f89b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9d4da7cd60>"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "COMPLAINTS_FN = 'complaints_sample.csv'\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Ykp1Qqnu5f"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIfFnl3z37VZ",
        "outputId": "236d2af2-0a77-4439-c16f-6393863a6c81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6624"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv \n",
        "#there are some records that have multiple lines per record (csv reader can handle this, but spark cannot handle this)\n",
        "#make sure that the when using spark, you get 6624 records\n",
        "len(list(csv.reader(open(COMPLAINTS_FN,'r'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usUd8mOS6SGB",
        "outputId": "be026463-20f2-4dbb-eae1-ab04e92562b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via', 'Date sent to company', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Complaint ID']\n"
          ]
        }
      ],
      "source": [
        "#getting headers\n",
        "with open(COMPLAINTS_FN, 'r') as file:\n",
        "  data = csv.reader(file)\n",
        "  for row in data:\n",
        "    print(row)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L6RhcgeK4-fn"
      },
      "outputs": [],
      "source": [
        "#function to handle multiline csv reading \n",
        "def extractData(partId, records):\n",
        "  if partId == 0:\n",
        "    next(records)\n",
        "  import csv\n",
        "  reader = csv.reader(records)\n",
        "  for row in reader:\n",
        "    if len(row) == 18:\n",
        "      yield row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6KbCuJfV5TqU"
      },
      "outputs": [],
      "source": [
        "#loading data into rdd using extractData function\n",
        "data = sc.textFile(COMPLAINTS_FN, use_unicode=True).cache().mapPartitionsWithIndex(extractData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kBrepe958RG",
        "outputId": "23598d1d-47ac-4895-b4a5-93e7d50e4540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|Date received|             Product|         Sub-product|               Issue|           Sub-issue|Consumer complaint narrative|Company public response|             Company|State|ZIP code|          Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|Complaint ID|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|   2015-12-31|Bank account or s...|    Checking account|Making/receiving ...|                    |                            |                       |FIRSTBANK PUERTO ...|   PR|   00902|Older American|                       N/A|     Referral|          2016-02-04|        Closed with expla...|             Yes|                No|     1723943|\n",
            "|   2016-03-15|Bank account or s...|Other bank produc...|Problems caused b...|                    |                            |                       |FIRSTBANK PUERTO ...|   PR|   00926|              |      Consent not provided|          Web|          2016-03-15|        Closed with expla...|             Yes|                No|     1833740|\n",
            "|   2016-10-24|Bank account or s...|    Checking account|Account opening, ...|                    |        In the month of X...|   Company has respo...|WELLS FARGO & COM...|   PR|   00969|              |          Consent provided|          Web|          2016-12-28|        Closed with non-m...|             Yes|                No|     2175792|\n",
            "|   2017-09-08|Checking or savin...|    Checking account| Managing an account|Deposits and with...|                            |                       |            Comerica|   TX|   77551|              |                       N/A|     Referral|          2017-11-07|        Closed with expla...|             Yes|               N/A|     2668920|\n",
            "|   2018-09-19|Checking or savin...|    Checking account|Problem with a le...|Money was taken f...|                            |   Company has respo...|WELLS FARGO & COM...|   FL|   33326|              |      Consent not provided|          Web|          2018-09-19|        Closed with expla...|             Yes|               N/A|     3023007|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = data.toDF(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via', 'Date sent to company', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Complaint ID'])\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByRDTZvu4DUe",
        "outputId": "42a8b303-7199-42b9-9f64-9a58b23221ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+--------------------+--------------------+--------------------+---------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|year|             Product|         Sub-product|               Issue|Sub-issue|Consumer complaint narrative|Company public response|             Company|State|ZIP code|          Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|Complaint ID|\n",
            "+----+--------------------+--------------------+--------------------+---------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|2015|bank account or s...|    Checking account|Making/receiving ...|         |                            |                       |FIRSTBANK PUERTO ...|   PR|   00902|Older American|                       N/A|     Referral|          2016-02-04|        Closed with expla...|             Yes|                No|     1723943|\n",
            "|2016|bank account or s...|Other bank produc...|Problems caused b...|         |                            |                       |FIRSTBANK PUERTO ...|   PR|   00926|              |      Consent not provided|          Web|          2016-03-15|        Closed with expla...|             Yes|                No|     1833740|\n",
            "|2016|bank account or s...|    Checking account|Account opening, ...|         |        In the month of X...|   Company has respo...|WELLS FARGO & COM...|   PR|   00969|              |          Consent provided|          Web|          2016-12-28|        Closed with non-m...|             Yes|                No|     2175792|\n",
            "+----+--------------------+--------------------+--------------------+---------+----------------------------+-----------------------+--------------------+-----+--------+--------------+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#converting Date received column to year, and convert product to all lowercase \n",
        "dfA = df.withColumn('Date received', F.split('Date received','-')[0]) \\\n",
        "  .withColumnRenamed('Date received','year') \\\n",
        "  .withColumn('Product', F.lower('Product'))\n",
        "\n",
        "dfA.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBwFcC4j4FEQ",
        "outputId": "6997d3a2-4c9e-4341-d230-79174baae820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+----------------+---------------+\n",
            "|             Product|year|Total Complaints|Total Companies|\n",
            "+--------------------+----+----------------+---------------+\n",
            "|bank account or s...|2015|               1|              1|\n",
            "|bank account or s...|2016|               2|              2|\n",
            "|checking or savin...|2017|               1|              1|\n",
            "|checking or savin...|2018|              20|             10|\n",
            "|checking or savin...|2019|             461|             72|\n",
            "|checking or savin...|2020|               3|              3|\n",
            "|       consumer loan|2015|               1|              1|\n",
            "|       consumer loan|2016|               1|              1|\n",
            "|       consumer loan|2017|               1|              1|\n",
            "|         credit card|2016|               4|              4|\n",
            "|         credit card|2017|               1|              1|\n",
            "|credit card or pr...|2017|               1|              1|\n",
            "|credit card or pr...|2018|              27|             12|\n",
            "|credit card or pr...|2019|             437|             42|\n",
            "|credit card or pr...|2020|              13|             10|\n",
            "|credit reporting,...|2017|               7|              5|\n",
            "|credit reporting,...|2018|             238|             22|\n",
            "|credit reporting,...|2019|            3114|            203|\n",
            "|credit reporting,...|2020|             144|             10|\n",
            "|     debt collection|2015|               4|              3|\n",
            "+--------------------+----+----------------+---------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#grouping by product and year; counting total number of complaints, and unique number of companies \n",
        "dfB = dfA.groupBy(['Product','year']).agg(F.count('*').alias('Total Complaints'),\\\n",
        "                                          F.countDistinct('Company').alias('Total Companies'))\\\n",
        "                                          .sort('Product','year')\n",
        "dfB.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHEkAGdD4HVg",
        "outputId": "cd9b6173-714e-4e77-a1d6-7522dd5617dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+--------------------+----------------+\n",
            "|             Product|year|             Company|Total Complaints|\n",
            "+--------------------+----+--------------------+----------------+\n",
            "|bank account or s...|2015|FIRSTBANK PUERTO ...|               1|\n",
            "|bank account or s...|2016|WELLS FARGO & COM...|               1|\n",
            "|bank account or s...|2016|FIRSTBANK PUERTO ...|               1|\n",
            "|checking or savin...|2017|            Comerica|               1|\n",
            "|checking or savin...|2018|REGIONS FINANCIAL...|               2|\n",
            "|checking or savin...|2018|NAVY FEDERAL CRED...|               3|\n",
            "|checking or savin...|2018|JPMORGAN CHASE & CO.|               5|\n",
            "|checking or savin...|2018|       PNC Bank N.A.|               1|\n",
            "|checking or savin...|2018|WELLS FARGO & COM...|               1|\n",
            "|checking or savin...|2018|            Comerica|               1|\n",
            "|checking or savin...|2018|UNITED SERVICES A...|               1|\n",
            "|checking or savin...|2018|BANK OF AMERICA, ...|               3|\n",
            "|checking or savin...|2018|BBVA FINANCIAL CO...|               2|\n",
            "|checking or savin...|2018|HSBC NORTH AMERIC...|               1|\n",
            "|checking or savin...|2019|CADENCE BANCORPOR...|               2|\n",
            "|checking or savin...|2019|Allied Interstate...|               1|\n",
            "|checking or savin...|2019|CHARLES SCHWAB CO...|               1|\n",
            "|checking or savin...|2019|  FIRST HORIZON BANK|               3|\n",
            "|checking or savin...|2019|          IBERIABANK|               2|\n",
            "|checking or savin...|2019|BANCO SANTANDER P...|               1|\n",
            "+--------------------+----+--------------------+----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#create a table grouped by product, year, and company so that we can calculate the sum complaints for each company\n",
        "dfC = dfA.groupBy(['Product','year','Company']).agg(F.count('*')\\\n",
        "        .alias('Total Complaints')).sort('Product','year')\n",
        "dfC.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0vTkFqv4JuJ",
        "outputId": "680cad34-a2ae-49ba-d9ec-a98a6f2100e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+---------------------+\n",
            "|             Product|year|max(Total Complaints)|\n",
            "+--------------------+----+---------------------+\n",
            "|bank account or s...|2015|                    1|\n",
            "|bank account or s...|2016|                    1|\n",
            "|checking or savin...|2017|                    1|\n",
            "|checking or savin...|2018|                    5|\n",
            "|checking or savin...|2019|                   62|\n",
            "|checking or savin...|2020|                    1|\n",
            "|       consumer loan|2015|                    1|\n",
            "|       consumer loan|2016|                    1|\n",
            "|       consumer loan|2017|                    1|\n",
            "|         credit card|2016|                    1|\n",
            "|         credit card|2017|                    1|\n",
            "|credit card or pr...|2017|                    1|\n",
            "|credit card or pr...|2018|                    9|\n",
            "|credit card or pr...|2019|                   66|\n",
            "|credit card or pr...|2020|                    3|\n",
            "|credit reporting,...|2017|                    2|\n",
            "|credit reporting,...|2018|                  134|\n",
            "|credit reporting,...|2019|                 1542|\n",
            "|credit reporting,...|2020|                   73|\n",
            "|     debt collection|2015|                    2|\n",
            "+--------------------+----+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#find the max total complaints for each product, year pair \n",
        "dfE = dfC.groupBy(['Product','year']).agg(F.max('Total Complaints')).sort('Product','year')\n",
        "dfE.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJ-bLq_94MbD",
        "outputId": "69932980-4630-4a52-a96e-2e5aa758fd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+----------------+---------------+---------------------+\n",
            "|             Product|year|Total Complaints|Total Companies|max(Total Complaints)|\n",
            "+--------------------+----+----------------+---------------+---------------------+\n",
            "|payday loan, titl...|2018|               7|              2|                    6|\n",
            "|checking or savin...|2020|               3|              3|                    1|\n",
            "|     debt collection|2019|            1130|            400|                   72|\n",
            "|credit card or pr...|2017|               1|              1|                    1|\n",
            "|        student loan|2019|             157|             37|                   58|\n",
            "|       consumer loan|2016|               1|              1|                    1|\n",
            "|            mortgage|2019|             415|             98|                   40|\n",
            "|money transfer, v...|2017|               1|              1|                    1|\n",
            "|        student loan|2020|               1|              1|                    1|\n",
            "|bank account or s...|2016|               2|              2|                    1|\n",
            "|checking or savin...|2018|              20|             10|                    5|\n",
            "|credit card or pr...|2018|              27|             12|                    9|\n",
            "|payday loan, titl...|2020|               1|              1|                    1|\n",
            "|     debt collection|2018|              47|             33|                    7|\n",
            "|            mortgage|2015|               1|              1|                    1|\n",
            "|money transfer, v...|2019|              87|             33|                   29|\n",
            "|checking or savin...|2019|             461|             72|                   62|\n",
            "|credit reporting,...|2020|             144|             10|                   73|\n",
            "|        student loan|2018|               2|              2|                    1|\n",
            "|vehicle loan or l...|2019|              90|             47|                    6|\n",
            "+--------------------+----+----------------+---------------+---------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#joining the tables to calculate the last column of highest percent of total complaints filed against one company for each product/year \n",
        "dfF = dfB.join(dfE, (dfB.Product == dfE.Product) & (dfB.year == dfE.year)).select(dfB['*'],dfE['max(Total Complaints)'])\n",
        "dfF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXUYUMHt4OlN",
        "outputId": "5db63e3a-6b7b-4df7-b809-fbb55e6e5302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+----+----------------+---------------+--------------------------------+\n",
            "|             Product|year|Total Complaints|Total Companies|Highest Perc of Total Complaints|\n",
            "+--------------------+----+----------------+---------------+--------------------------------+\n",
            "|bank account or s...|2015|               1|              1|                             100|\n",
            "|bank account or s...|2016|               2|              2|                              50|\n",
            "|checking or savin...|2017|               1|              1|                             100|\n",
            "|checking or savin...|2018|              20|             10|                              25|\n",
            "|checking or savin...|2019|             461|             72|                              13|\n",
            "|checking or savin...|2020|               3|              3|                              33|\n",
            "|       consumer loan|2015|               1|              1|                             100|\n",
            "|       consumer loan|2016|               1|              1|                             100|\n",
            "|       consumer loan|2017|               1|              1|                             100|\n",
            "|         credit card|2016|               4|              4|                              25|\n",
            "|         credit card|2017|               1|              1|                             100|\n",
            "|credit card or pr...|2017|               1|              1|                             100|\n",
            "|credit card or pr...|2018|              27|             12|                              33|\n",
            "|credit card or pr...|2019|             437|             42|                              15|\n",
            "|credit card or pr...|2020|              13|             10|                              23|\n",
            "|credit reporting,...|2017|               7|              5|                              29|\n",
            "|credit reporting,...|2018|             238|             22|                              56|\n",
            "|credit reporting,...|2019|            3114|            203|                              50|\n",
            "|credit reporting,...|2020|             144|             10|                              51|\n",
            "|     debt collection|2015|               4|              3|                              50|\n",
            "+--------------------+----+----------------+---------------+--------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#calculating the highest percent of total complaints, rounding to the nearest percent and casting to integer\n",
        "dfG = dfF.withColumn('Highest Perc of Total Complaints',F.round(dfF['max(Total Complaints)']/dfF['Total Complaints']*100).cast('integer'))\\\n",
        "      .select('Product','year','Total Complaints','Total Companies','Highest Perc of Total Complaints').sort('Product','year')\n",
        "\n",
        "#converting all columns to strings \n",
        "dfG = dfG.select([dfG[c].cast('string') for c in dfG.columns])\n",
        "\n",
        "dfG.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9b8NN-cW26J",
        "outputId": "cc6b0b88-1118-40f6-d0c7-2191ae8603c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bank account or service,2015,1,1,100',\n",
              " 'bank account or service,2016,2,2,50',\n",
              " 'checking or savings account,2017,1,1,100',\n",
              " 'checking or savings account,2018,20,10,25',\n",
              " 'checking or savings account,2019,461,72,13',\n",
              " 'checking or savings account,2020,3,3,33',\n",
              " 'consumer loan,2015,1,1,100',\n",
              " 'consumer loan,2016,1,1,100',\n",
              " 'consumer loan,2017,1,1,100',\n",
              " 'credit card,2016,4,4,25',\n",
              " 'credit card,2017,1,1,100',\n",
              " 'credit card or prepaid card,2017,1,1,100',\n",
              " 'credit card or prepaid card,2018,27,12,33',\n",
              " 'credit card or prepaid card,2019,437,42,15',\n",
              " 'credit card or prepaid card,2020,13,10,23',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2017,7,5,29',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2018,238,22,56',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2019,3114,203,50',\n",
              " 'credit reporting, credit repair services, or other personal consumer reports,2020,144,10,51',\n",
              " 'debt collection,2015,4,3,50']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# outputTask1 is an output RDD, you can use DataFrame as well but each line\n",
        "# still needs to be a string\n",
        "outputTask1 = dfG.rdd.map(lambda x: ','.join(x))\n",
        "outputTask1.take(20)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oNXJMMLEEcHB"
      },
      "source": [
        "## Task 2\n",
        "\n",
        "For this task, task 1 will be converted into a standalone file that can be run on any DataProc cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJecoKVsqJcJ",
        "outputId": "1e0338db-c352-4472-acef-0897d4d08533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting BDM_HW3_24363838_Lau.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile BDM_HW3_24363838_Lau.py\n",
        "#!/usr/bin/python\n",
        "\n",
        "#importing libraries \n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import sys \n",
        "\n",
        "#spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "sc = pyspark.SparkContext.getOrCreate()\n",
        "\n",
        "#defining the input path \n",
        "path = sys.argv[1]\n",
        "\n",
        "\n",
        "#function to handle multiline csv reading \n",
        "def extractData(partId, records):\n",
        "  if partId == 0:\n",
        "    next(records)\n",
        "  import csv\n",
        "  reader = csv.reader(records)\n",
        "  for row in reader:\n",
        "    if len(row) == 18:\n",
        "      yield row\n",
        "\n",
        "\n",
        "#loading data into rdd using extractData function\n",
        "data = sc.textFile(path, use_unicode=True).cache().mapPartitionsWithIndex(extractData)\n",
        "\n",
        "#converting to pyspark dataframe to compute output \n",
        "df = data.toDF(['Date received', 'Product', 'Sub-product', 'Issue', 'Sub-issue', 'Consumer complaint narrative', 'Company public response', 'Company', 'State', 'ZIP code', 'Tags', 'Consumer consent provided?', 'Submitted via', 'Date sent to company', 'Company response to consumer', 'Timely response?', 'Consumer disputed?', 'Complaint ID'])\n",
        "\n",
        "print(df.count())\n",
        "\n",
        "#reading in the csv data, retrieving only the year for date, and making all product names lowercase \n",
        "df = df.withColumn('Date received', F.split('Date received','-')[0]) \\\n",
        "  .withColumnRenamed('Date received','year') \\\n",
        "  .withColumn('Product', F.lower('Product'))\n",
        "\n",
        "#find the total complaints and total companies by product, year \n",
        "dfA = df.groupBy(['Product','year']).agg(F.count('*').alias('Total Complaints'),\\\n",
        "                                        F.countDistinct('Company').alias('Total Companies'))\n",
        "\n",
        "#find the max total complaints by product, year when grouped by product, year, and company \n",
        "dfB = df.groupBy(['Product','year','Company']).agg(F.count('*')\\\n",
        "        .alias('Total Complaints')).groupBy(['Product','year']).agg(F.max('Total Complaints'))\n",
        "\n",
        "#joining tables \n",
        "dfC = dfA.join(dfB, (dfA.Product == dfB.Product) & (dfA.year == dfB.year)).select(dfA['*'],dfB['max(Total Complaints)'])\n",
        "\n",
        "#calculating the percentage, formatting table for output \n",
        "output = dfC.withColumn('Highest Perc of Total Complaints',F.round(dfC['max(Total Complaints)']/dfC['Total Complaints']*100).cast('integer'))\\\n",
        "      .select('Product','year','Total Complaints','Total Companies','Highest Perc of Total Complaints').sort('Product','year')\n",
        "\n",
        "#converting columns to string, and converting into rdd containing strings with comma separated values \n",
        "output = output.select([output[c].cast('string') for c in output.columns]).rdd.map(lambda x: ','.join(x))\n",
        "\n",
        "#saving as csv \n",
        "output.saveAsTextFile(sys.argv[2])\n",
        "\n",
        "print(output.count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gepJBSg9f50G",
        "outputId": "0398bd31-cecf-4c57-9896-88e0419e3f5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/04/18 16:01:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "23/04/18 16:01:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "23/04/18 16:01:11 WARN BlockManager: Task 0 already completed, not releasing lock for rdd_1_0\n",
            "6623\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BDM_HW3_24363838_Lau.py\", line 60, in <module>\n",
            "    output.saveAsTextFile(sys.argv[2])\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\", line 3406, in saveAsTextFile\n",
            "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o160.saveAsTextFile.\n",
            ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/content/testingCode already exists\n",
            "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
            "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n",
            "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1579)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
            "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1579)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:564)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:563)\n",
            "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#testing on sample_complaints.csv\n",
        "!python BDM_HW3_24363838_Lau.py /content/complaints_sample.csv /content/testingCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3PzHjtJgFpB",
        "outputId": "2e1298d8-2325-49b4-b469-91e058ff276b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: google-cloud-dataproc in /usr/local/lib/python3.9/dist-packages (5.4.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (1.22.2)\n",
            "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (2.11.0)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.9/dist-packages (from google-cloud-dataproc) (0.12.6)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.14.1 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.17.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.27.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.59.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.53.0)\n",
            "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.9/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (5.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.16.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (4.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (1.26.15)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.14.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0->google-cloud-dataproc) (0.4.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-cloud-dataproc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep1Y2Rn0lS6h",
        "outputId": "74ee2330-5edc-4eb7-9672-d57491786b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=VIxgRg9afXP8UhRtIFD1gjKTABG3kX&prompt=consent&access_type=offline&code_challenge=RvJstd50YMI21xqO9YKsfvhdd32krZKXJcEUKm-0Te8&code_challenge_method=S256\n",
            "\n",
            "Enter authorization code: 4/0AVHEtk5ly9MtOnV5afaewKFvNc2RR9l_Ca9LzI-QsxqPyoSOJ7mKqWt4DirTcnPalyQ02g\n",
            "\n",
            "You are now logged in as [alau002@citymail.cuny.edu].\n",
            "Your current project is [bigdata-380720].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1kckjOZlaLn",
        "outputId": "7ddeacf3-5873-4950-8c3c-c11592edc812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ID      NAME     PROJECT_NUMBER\n",
            "bigdata-380720  BigData  267580964279\n"
          ]
        }
      ],
      "source": [
        "!gcloud projects list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M-nX_98ldO5",
        "outputId": "2e3e3f16-f193-46ea-c63f-dead1dc62745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Updated property [compute/region].\n",
            "Updated property [compute/zone].\n",
            "Updated property [dataproc/region].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project bigdata-380720\n",
        "!gcloud config set compute/region us-west1\n",
        "!gcloud config set compute/zone us-west1-a\n",
        "!gcloud config set dataproc/region us-west1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7dmf--0lh9t",
        "outputId": "5276c3c2-e09b-4ded-95b3-386d895fcd98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting on operation [projects/bigdata-380720/regions/us-west1/operations/2459d235-c514-3258-ad27-7d21679a37e1].\n",
            "\n",
            "\u001b[1;33mWARNING:\u001b[0m Consider using Auto Zone rather than selecting a zone manually. See https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/auto-zone\n",
            "\u001b[1;33mWARNING:\u001b[0m Failed to validate permissions required for default service account: '267580964279-compute@developer.gserviceaccount.com'. Cluster creation could still be successful if required permissions have been granted to the respective service accounts as mentioned in the document https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2. This could be due to Cloud Resource Manager API hasn't been enabled in your project '267580964279' before or it is disabled. Enable it by visiting 'https://console.developers.google.com/apis/api/cloudresourcemanager.googleapis.com/overview?project=267580964279'.\n",
            "\u001b[1;33mWARNING:\u001b[0m For PD-Standard without local SSDs, we strongly recommend provisioning 1TB or larger to ensure consistently high I/O performance. See https://cloud.google.com/compute/docs/disks/performance for information on disk I/O performance.\n",
            "Created [https://dataproc.googleapis.com/v1/projects/bigdata-380720/regions/us-west1/clusters/bdm-hw3] Cluster placed in zone [us-west1-a].\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters create bdm-hw3 --enable-component-gateway --region us-west1 --zone us-west1-a --master-machine-type n1-standard-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-4 --worker-boot-disk-size 500 --image-version 2.0-debian10 --project bigdata-380720"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTPlavHHlk_n",
        "outputId": "5c751c63-d3b1-4b6e-9f57-8c9afd23d4cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NAME     PLATFORM  WORKER_COUNT  PREEMPTIBLE_WORKER_COUNT  STATUS   ZONE        SCHEDULED_DELETE\n",
            "bdm-hw3  GCE       2                                       RUNNING  us-west1-a\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot5W2i19nYOB",
        "outputId": "e8d64cf7-5cad-4f3e-f86b-1e9b3c69443b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Job [e144875e79ba47f1a00f6878d8a4f3f9] submitted.\n",
            "Waiting for job output...\n",
            "23/04/18 16:04:22 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
            "23/04/18 16:04:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
            "23/04/18 16:04:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "23/04/18 16:04:22 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
            "23/04/18 16:04:22 INFO org.sparkproject.jetty.util.log: Logging initialized @4852ms to org.sparkproject.jetty.util.log.Slf4jLog\n",
            "23/04/18 16:04:22 INFO org.sparkproject.jetty.server.Server: jetty-9.4.40.v20210413; built: 2021-04-13T20:42:42.668Z; git: b881a572662e1943a14ae12e7e1207989f218b74; jvm 1.8.0_362-b09\n",
            "23/04/18 16:04:22 INFO org.sparkproject.jetty.server.Server: Started @4946ms\n",
            "23/04/18 16:04:22 INFO org.sparkproject.jetty.server.AbstractConnector: Started ServerConnector@3e49a872{HTTP/1.1, (http/1.1)}{0.0.0.0:42669}\n",
            "23/04/18 16:04:23 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.20:8032\n",
            "23/04/18 16:04:23 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at bdm-hw3-m/10.138.0.20:10200\n",
            "23/04/18 16:04:24 INFO org.apache.hadoop.conf.Configuration: resource-types.xml not found\n",
            "23/04/18 16:04:24 INFO org.apache.hadoop.yarn.util.resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
            "23/04/18 16:04:26 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1681833803903_0001\n",
            "23/04/18 16:04:27 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at bdm-hw3-m/10.138.0.20:8030\n",
            "23/04/18 16:04:29 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl: Ignoring exception of type GoogleJsonResponseException; verified object already exists with desired state.\n",
            "23/04/18 16:04:31 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\n",
            "3470438\n",
            "23/04/18 16:06:15 INFO com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageFileSystem: Successfully repaired 'gs://bdma/shared/2023_spring/HW3/24363838_Lau/' directory.\n",
            "125\n",
            "23/04/18 16:06:16 INFO org.sparkproject.jetty.server.AbstractConnector: Stopped Spark@3e49a872{HTTP/1.1, (http/1.1)}{0.0.0.0:0}\n",
            "Job [e144875e79ba47f1a00f6878d8a4f3f9] finished successfully.\n",
            "done: true\n",
            "driverControlFilesUri: gs://dataproc-staging-us-west1-267580964279-2vdinbhx/google-cloud-dataproc-metainfo/7236caf7-92b6-4188-ab4e-8173e2520c54/jobs/e144875e79ba47f1a00f6878d8a4f3f9/\n",
            "driverOutputResourceUri: gs://dataproc-staging-us-west1-267580964279-2vdinbhx/google-cloud-dataproc-metainfo/7236caf7-92b6-4188-ab4e-8173e2520c54/jobs/e144875e79ba47f1a00f6878d8a4f3f9/driveroutput\n",
            "jobUuid: 43184892-e342-3760-85b6-1da3b22222c4\n",
            "placement:\n",
            "  clusterName: bdm-hw3\n",
            "  clusterUuid: 7236caf7-92b6-4188-ab4e-8173e2520c54\n",
            "pysparkJob:\n",
            "  args:\n",
            "  - gs://bdma/data/complaints.csv\n",
            "  - gs://bdma/shared/2023_spring/HW3/24363838_Lau\n",
            "  mainPythonFileUri: gs://dataproc-staging-us-west1-267580964279-2vdinbhx/google-cloud-dataproc-metainfo/7236caf7-92b6-4188-ab4e-8173e2520c54/jobs/e144875e79ba47f1a00f6878d8a4f3f9/staging/BDM_HW3_24363838_Lau.py\n",
            "reference:\n",
            "  jobId: e144875e79ba47f1a00f6878d8a4f3f9\n",
            "  projectId: bigdata-380720\n",
            "status:\n",
            "  state: DONE\n",
            "  stateStartTime: '2023-04-18T16:06:21.063425Z'\n",
            "statusHistory:\n",
            "- state: PENDING\n",
            "  stateStartTime: '2023-04-18T16:04:15.376396Z'\n",
            "- state: SETUP_DONE\n",
            "  stateStartTime: '2023-04-18T16:04:15.429149Z'\n",
            "- details: Agent reported job success\n",
            "  state: RUNNING\n",
            "  stateStartTime: '2023-04-18T16:04:15.733960Z'\n",
            "yarnApplications:\n",
            "- name: BDM_HW3_24363838_Lau.py\n",
            "  progress: 1.0\n",
            "  state: FINISHED\n",
            "  trackingUrl: http://bdm-hw3-m:8088/proxy/application_1681833803903_0001/\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc jobs submit pyspark --cluster bdm-hw3 BDM_HW3_24363838_Lau.py -- gs://bdma/data/complaints.csv gs://bdma/shared/2023_spring/HW3/24363838_Lau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBo5bCNQn0Ih",
        "outputId": "09383b42-b7aa-4d1a-a9d7-0bdeae8fa472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gs://bdma/shared/2023_spring/HW3/14211712_Salas/\n",
            "gs://bdma/shared/2023_spring/HW3/24363838_Lau/\n",
            "gs://bdma/shared/2023_spring/HW3/24373710_Uddin/\n",
            "gs://bdma/shared/2023_spring/HW3/24438996_Radaelli/\n"
          ]
        }
      ],
      "source": [
        "!gsutil ls gs://bdma/shared/2023_spring/HW3/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfq8hGzfEEwb",
        "outputId": "aa133baf-1dfc-4947-8d8f-e18527a38dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bank account or service,2012,12212,98,19\n",
            "bank account or service,2013,13388,164,18\n",
            "bank account or service,2014,14662,258,17\n",
            "bank account or service,2015,17140,215,17\n",
            "bank account or service,2016,21848,230,15\n",
            "bank account or service,2017,6955,173,16\n",
            "checking or savings account,2017,12763,183,17\n",
            "checking or savings account,2018,21211,214,16\n",
            "checking or savings account,2019,21735,249,15\n",
            "checking or savings account,2020,24238,269,14\n",
            "checking or savings account,2021,29555,289,14\n",
            "checking or savings account,2022,37585,346,14\n",
            "checking or savings account,2023,12279,224,37\n",
            "consumer loan,2012,1986,84,19\n",
            "consumer loan,2013,3117,159,12\n",
            "consumer loan,2014,5456,357,8\n",
            "consumer loan,2015,7882,596,9\n",
            "consumer loan,2016,9591,664,7\n",
            "consumer loan,2017,3544,424,8\n",
            "credit card,2011,1260,33,19\n",
            "credit card,2012,15353,76,20\n",
            "credit card,2013,13105,108,19\n",
            "credit card,2014,13974,178,17\n",
            "credit card,2015,17300,225,17\n",
            "credit card,2016,21065,221,21\n",
            "credit card,2017,7133,127,18\n",
            "credit card or prepaid card,2017,15404,170,15\n",
            "credit card or prepaid card,2018,24243,250,15\n",
            "credit card or prepaid card,2019,25827,241,15\n",
            "credit card or prepaid card,2020,33884,300,15\n",
            "credit card or prepaid card,2021,31822,320,13\n",
            "credit card or prepaid card,2022,39886,367,12\n",
            "credit card or prepaid card,2023,10388,200,12\n",
            "credit reporting,2012,1873,31,38\n",
            "credit reporting,2013,14380,203,36\n",
            "credit reporting,2014,29238,213,35\n",
            "credit reporting,2015,34272,237,35\n",
            "credit reporting,2016,44081,182,36\n",
            "credit reporting,2017,16585,401,30\n",
            "credit reporting, credit repair services, or other personal consumer reports,2017,73384,1015,35\n",
            "credit reporting, credit repair services, or other personal consumer reports,2018,111609,1301,26\n",
            "credit reporting, credit repair services, or other personal consumer reports,2019,138564,1319,29\n",
            "credit reporting, credit repair services, or other personal consumer reports,2020,283735,1441,31\n",
            "credit reporting, credit repair services, or other personal consumer reports,2021,307553,1645,48\n",
            "credit reporting, credit repair services, or other personal consumer reports,2022,604227,1642,31\n",
            "credit reporting, credit repair services, or other personal consumer reports,2023,204666,914,33\n",
            "debt collection,2013,11069,984,9\n",
            "debt collection,2014,39127,1754,6\n",
            "debt collection,2015,39717,2133,5\n",
            "debt collection,2016,40452,2131,4\n",
            "debt collection,2017,47915,2229,4\n",
            "debt collection,2018,51144,2236,5\n",
            "debt collection,2019,46373,2106,4\n",
            "debt collection,2020,53921,2098,5\n",
            "debt collection,2021,70340,2218,5\n",
            "debt collection,2022,59502,2104,4\n",
            "debt collection,2023,14112,1199,4\n",
            "money transfer, virtual currency, or money service,2017,3266,178,32\n",
            "money transfer, virtual currency, or money service,2018,5432,234,30\n",
            "money transfer, virtual currency, or money service,2019,5085,263,26\n",
            "money transfer, virtual currency, or money service,2020,8308,274,23\n",
            "money transfer, virtual currency, or money service,2021,13895,332,34\n",
            "money transfer, virtual currency, or money service,2022,13537,361,19\n",
            "money transfer, virtual currency, or money service,2023,2829,188,24\n",
            "money transfers,2013,559,58,30\n",
            "money transfers,2014,1169,67,32\n",
            "money transfers,2015,1619,79,32\n",
            "money transfers,2016,1567,80,25\n",
            "money transfers,2017,440,46,27\n",
            "mortgage,2011,1276,66,33\n",
            "mortgage,2012,38108,381,31\n",
            "mortgage,2013,49398,454,25\n",
            "mortgage,2014,42961,495,14\n",
            "mortgage,2015,42345,680,12\n",
            "mortgage,2016,41466,766,14\n",
            "mortgage,2017,30574,770,12\n",
            "mortgage,2018,24571,697,12\n",
            "mortgage,2019,22706,668,9\n",
            "mortgage,2020,24658,769,8\n",
            "mortgage,2021,26531,773,7\n",
            "mortgage,2022,23301,722,7\n",
            "mortgage,2023,5933,339,29\n",
            "other financial service,2014,116,38,12\n",
            "other financial service,2015,312,99,10\n",
            "other financial service,2016,465,154,5\n",
            "other financial service,2017,165,75,7\n",
            "payday loan,2013,194,31,13\n",
            "payday loan,2014,1706,159,17\n",
            "payday loan,2015,1585,243,8\n",
            "payday loan,2016,1563,213,17\n",
            "payday loan,2017,493,107,12\n",
            "payday loan, title loan, or personal loan,2017,2949,390,6\n",
            "payday loan, title loan, or personal loan,2018,4361,466,5\n",
            "payday loan, title loan, or personal loan,2019,4320,482,4\n",
            "payday loan, title loan, or personal loan,2020,4207,485,4\n",
            "payday loan, title loan, or personal loan,2021,4353,465,4\n",
            "payday loan, title loan, or personal loan,2022,5772,541,4\n",
            "payday loan, title loan, or personal loan,2023,1393,273,6\n",
            "prepaid card,2014,336,29,22\n",
            "prepaid card,2015,1784,48,42\n",
            "prepaid card,2016,1250,51,21\n",
            "prepaid card,2017,449,30,19\n",
            "student loan,2012,2840,53,42\n",
            "student loan,2013,3005,91,46\n",
            "student loan,2014,4283,137,44\n",
            "student loan,2015,4501,171,37\n",
            "student loan,2016,8087,232,34\n",
            "student loan,2017,17173,232,63\n",
            "student loan,2018,8776,201,46\n",
            "student loan,2019,7212,192,41\n",
            "student loan,2020,4388,182,33\n",
            "student loan,2021,4131,181,29\n",
            "student loan,2022,7956,155,20\n",
            "student loan,2023,1366,64,43\n",
            "vehicle loan or lease,2017,3695,250,13\n",
            "vehicle loan or lease,2018,5887,311,12\n",
            "vehicle loan or lease,2019,5488,324,12\n",
            "vehicle loan or lease,2020,6975,306,13\n",
            "vehicle loan or lease,2021,7826,332,12\n",
            "vehicle loan or lease,2022,8655,352,8\n",
            "vehicle loan or lease,2023,3239,190,30\n",
            "virtual currency,2014,1,1,100\n",
            "virtual currency,2015,7,1,100\n",
            "virtual currency,2016,7,2,57\n",
            "virtual currency,2017,3,1,100\n"
          ]
        }
      ],
      "source": [
        "!gsutil cat gs://bdma/shared/2023_spring/HW3/24363838_Lau/part*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMckUyAXC2r1",
        "outputId": "7ad67ab7-9a78-4005-a206-f30ebed72a09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/#1681832440900976...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/_SUCCESS#1681832441163957...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00000#1681832438219404...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00001#1681832438059436...\n",
            "/ [4 objects]                                                                   \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m rm ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00002#1681832438154391...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00003#1681832438057270...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00004#1681832438226394...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00005#1681832438058054...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00006#1681832438154471...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00007#1681832438062768...\n",
            "Removing gs://bdma/shared/2023_spring/HW3/24363838_Lau/part-00008#1681832439699258...\n",
            "/ [11 objects]                                                                  \n",
            "Operation completed over 11 objects.                                             \n"
          ]
        }
      ],
      "source": [
        "#!gsutil rm -r gs://bdma/shared/2023_spring/HW3/24363838_Lau/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFPL8yOEloKO",
        "outputId": "2a38edf1-1889-43da-ebce-0a0ba5ec2e2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Waiting on operation [projects/bigdata-380720/regions/us-west1/operations/785b5652-7d49-39cf-8597-b228b5cbf1c5].\n",
            "Deleted [https://dataproc.googleapis.com/v1/projects/bigdata-380720/regions/us-west1/clusters/bdm-hw3].\n",
            "Listed 0 items.\n"
          ]
        }
      ],
      "source": [
        "!gcloud dataproc clusters delete bdm-hw3 -q\n",
        "!gcloud dataproc clusters list"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
